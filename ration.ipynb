{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_sub():\n",
    "    if not os.path.exists(\"../input2/sub.csv\"):\n",
    "        operation_UID = op_test['UID'].values.tolist()\n",
    "        transaction_UID = trans_test['UID'].values.tolist()\n",
    "        test_ids = list(set(operation_UID).union(set(transaction_UID)))\n",
    "        pd.DataFrame({\"UID\":test_ids}).to_csv(\"../input2/sub.csv\",index=False)\n",
    "        return pd.DataFrame({\"UID\":test_ids})\n",
    "    else:\n",
    "        return pd.read_csv(\"../input2/sub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdd/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "op_train = pd.read_csv('../input/operation_train_new.csv')\n",
    "trans_train = pd.read_csv('../input/transaction_train_new.csv')\n",
    "\n",
    "op_test = pd.read_csv('../input2/test_operation_round2.csv')\n",
    "trans_test = pd.read_csv('../input2/test_transaction_round2.csv')\n",
    "y = pd.read_csv('../input/tag_train_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(op,trans,label):\n",
    "    for feature in op.columns[:]:\n",
    "        if feature not in ['day']:\n",
    "            if feature != 'UID':\n",
    "                pass\n",
    "#                 label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "#                 label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2','geo_code']:\n",
    "                if feature not in deliver:\n",
    "                    if feature != 'UID':\n",
    "                        temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver+'_cnt']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver+'_nunique']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver+'_cnt']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver+'_nunique']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "\n",
    "        else:\n",
    "            print(feature)\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(op.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2']:\n",
    "                if feature not in deliver:\n",
    "                    temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver+'_cnt']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver+'_nunique']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "#                     temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "#                     temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "#                     temp.columns = ['UID',feature+deliver+'_max']\n",
    "#                     label =label.merge(temp,on='UID',how='left')\n",
    "#                     del temp\n",
    "#                     temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "#                     temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "#                     temp.columns = ['UID',feature+deliver+'_min']\n",
    "#                     label =label.merge(temp,on='UID',how='left')\n",
    "#                     del temp\n",
    "#                     temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "#                     temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "#                     temp.columns = ['UID',feature+deliver+'_sum']\n",
    "#                     label =label.merge(temp,on='UID',how='left')\n",
    "#                     del temp\n",
    "#                     temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "#                     temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "#                     temp.columns = ['UID',feature+deliver+'_mean']\n",
    "#                     label =label.merge(temp,on='UID',how='left')\n",
    "#                     del temp\n",
    "                    temp = op[['UID',deliver]].drop_duplicates().merge(op.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver+'_std']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    for feature in trans.columns[1:]:\n",
    "        if feature not in ['trans_amt','bal','day']:\n",
    "            if feature != 'UID':\n",
    "                pass\n",
    "#                 label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "#                 label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1','geo_code',]:\n",
    "                if feature not in deliver: \n",
    "                    if feature != 'UID':\n",
    "                        temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID','T_'+feature+deliver+'_cnt']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID','T_'+feature+deliver+'_nunique']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID','T_'+feature+deliver+'_cnt']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID','T_'+feature+deliver+'_nunique']\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "            #if feature in ['merchant','code2','acc_id1','market_code','market_code']:\n",
    "            #    label[feature+'_z'] = 0 \n",
    "            #    label[feature+'_z'] = label[feature+'_y']/label[feature+'_x']\n",
    "        else:\n",
    "            print(feature)\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "#             label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1']:\n",
    "                if feature not in deliver:\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_cnt']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_nunique']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_std']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    if feature == 'day':\n",
    "                        continue\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_max']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_min']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_sum']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].drop_duplicates().merge(trans.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID','T_'+feature+deliver+'_mean']\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    print(\"Done\")\n",
    "    return label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = get_feature(op_train,trans_train,y).fillna(-1)\n",
    "test = get_feature(op_test,trans_test,sub).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('../feature2/ration_train_m2.csv', index=False)\n",
    "test.to_csv('../feature2/ration_test_m2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ration_train = pd.read_csv('../feature2/ration_train.csv')\n",
    "ration_test = pd.read_csv('../feature2/ration_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.095158\tvalid_1's binary_logloss: 0.115989\n",
      "[100]\tvalid_0's binary_logloss: 0.0602824\tvalid_1's binary_logloss: 0.0931599\n",
      "[150]\tvalid_0's binary_logloss: 0.0458964\tvalid_1's binary_logloss: 0.0873905\n",
      "[200]\tvalid_0's binary_logloss: 0.0374556\tvalid_1's binary_logloss: 0.0857349\n",
      "[250]\tvalid_0's binary_logloss: 0.0317752\tvalid_1's binary_logloss: 0.0849276\n",
      "[300]\tvalid_0's binary_logloss: 0.0277924\tvalid_1's binary_logloss: 0.0851144\n",
      "Early stopping, best iteration is:\n",
      "[280]\tvalid_0's binary_logloss: 0.0292573\tvalid_1's binary_logloss: 0.0847479\n",
      "[0.08474791434044228]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.094432\tvalid_1's binary_logloss: 0.113211\n",
      "[100]\tvalid_0's binary_logloss: 0.06002\tvalid_1's binary_logloss: 0.0915104\n",
      "[150]\tvalid_0's binary_logloss: 0.0455257\tvalid_1's binary_logloss: 0.0867015\n",
      "[200]\tvalid_0's binary_logloss: 0.0372777\tvalid_1's binary_logloss: 0.085356\n",
      "[250]\tvalid_0's binary_logloss: 0.0316591\tvalid_1's binary_logloss: 0.0853542\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's binary_logloss: 0.0345312\tvalid_1's binary_logloss: 0.0852419\n",
      "[0.08474791434044228, 0.0852419456847227]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0930215\tvalid_1's binary_logloss: 0.119704\n",
      "[100]\tvalid_0's binary_logloss: 0.0580583\tvalid_1's binary_logloss: 0.100678\n",
      "[150]\tvalid_0's binary_logloss: 0.0441515\tvalid_1's binary_logloss: 0.0976751\n",
      "[200]\tvalid_0's binary_logloss: 0.0359254\tvalid_1's binary_logloss: 0.0972008\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's binary_logloss: 0.0389585\tvalid_1's binary_logloss: 0.0969905\n",
      "[0.08474791434044228, 0.0852419456847227, 0.09699052822154942]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.093232\tvalid_1's binary_logloss: 0.119652\n",
      "[100]\tvalid_0's binary_logloss: 0.0590432\tvalid_1's binary_logloss: 0.0985806\n",
      "[150]\tvalid_0's binary_logloss: 0.044966\tvalid_1's binary_logloss: 0.0937676\n",
      "[200]\tvalid_0's binary_logloss: 0.0367148\tvalid_1's binary_logloss: 0.0919019\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid_0's binary_logloss: 0.0343924\tvalid_1's binary_logloss: 0.0914484\n",
      "[0.08474791434044228, 0.0852419456847227, 0.09699052822154942, 0.09144842039343054]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.094637\tvalid_1's binary_logloss: 0.113541\n",
      "[100]\tvalid_0's binary_logloss: 0.0599142\tvalid_1's binary_logloss: 0.0930484\n",
      "[150]\tvalid_0's binary_logloss: 0.0455568\tvalid_1's binary_logloss: 0.0896646\n",
      "[200]\tvalid_0's binary_logloss: 0.0372493\tvalid_1's binary_logloss: 0.0889184\n",
      "[250]\tvalid_0's binary_logloss: 0.0315781\tvalid_1's binary_logloss: 0.0886697\n",
      "Early stopping, best iteration is:\n",
      "[269]\tvalid_0's binary_logloss: 0.0298668\tvalid_1's binary_logloss: 0.0884128\n",
      "[0.08474791434044228, 0.0852419456847227, 0.09699052822154942, 0.09144842039343054, 0.08841278120225042]\n",
      "0.808098016336056\n"
     ]
    }
   ],
   "source": [
    "train = train.drop(['Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']\n",
    "\n",
    "# test_id = test['UID']\n",
    "# test = test.drop(['Tag'],axis = 1).fillna(-1)\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=3, reg_lambda=5, max_depth=-1,\n",
    "    n_estimators=5000, objective='binary', subsample=0.9, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.05,\n",
    "    random_state=1000, n_jobs=16, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "# sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=30)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "#     test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "#     sub_preds += test_pred / 5\n",
    "    #print('test mean:', test_pred.mean())\n",
    "    #predict_result['predicted_score'] = predict_result['predicted_score'] + test_pred\n",
    "\n",
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdd/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/xdd/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sub.csv')\n",
    "sub['Tag'] = sub_preds\n",
    "sub.to_csv('../sub/baseline_%s.csv'%str(m),index=False)\n",
    "\n",
    "\n",
    "\n",
    "path = '../input/'\n",
    "trans_train = pd.read_csv(path+'transaction_train_new.csv')\n",
    "y = pd.read_csv(path+'tag_train_new.csv')\n",
    "trans_train = trans_train.merge(y,on='UID',how='left')\n",
    "def find_wrong(trans_train,y,feature):\n",
    "    black = (trans_train.groupby([feature])['Tag'].sum()/trans_train.groupby([feature])['Tag'].count()).sort_values(ascending=False)\n",
    "    tag_count = trans_train.groupby([feature])['Tag'].count().reset_index()\n",
    "    black = black.reset_index()\n",
    "    black = black.merge(tag_count,on=feature,how='left')\n",
    "    black = black.sort_values(by = ['Tag_x','Tag_y'],ascending=False)\n",
    "    return black\n",
    "\n",
    "Test_trans = pd.read_csv(path+'transaction_round1_new.csv')\n",
    "Test_tag = pd.read_csv('../sub/baseline_%s.csv'%str(m)) # 测试样本\n",
    "#rule_code = [  '5776870b5747e14e' ,'8b3f74a1391b5427' ,'0e90f47392008def' ,'6d55ccc689b910ee' ,'2260d61b622795fb' ,'1f72814f76a984fa' ,'c2e87787a76836e0' ,'4bca6018239c6201' ,'922720f3827ccef8' ,'2b2e7046145d9517' ,'09f911b8dc5dfc32' ,'7cc961258f4dce9c' ,'bc0213f01c5023ac' ,'0316dca8cc63cc17' ,'c988e79f00cc2dc0' ,'d0b1218bae116267' ,'72fac912326004ee' ,'00159b7cc2f1dfc8' ,'49ec5883ba0c1b0e' ,'c9c29fc3d44a1d7b' ,'33ce9c3877281764' ,'e7c929127cdefadb' ,'05bc3e22c112c8c9' ,'5cf4f55246093ccf' ,'6704d8d8d5965303' ,'4df1708c5827264d' ,'6e8b399ffe2d1e80' ,'f65104453e0b1d10' ,'1733ddb502eb3923' ,'a086f47f681ad851' ,'1d4372ca8a38cd1f' ,'29db08e2284ea103' ,'4e286438d39a6bd4' ,'54cb3985d0380ca4' ,'6b64437be7590eb0' ,'89eb97474a6cb3c6' ,'95d506c0e49a492c' ,'c17b47056178e2bb' ,'d36b25a74285bebb']\n",
    "\n",
    "black = find_wrong(trans_train,y,'merchant')\n",
    "rule_code_1 = black.sort_values(by=['Tag_x','Tag_y'],ascending=False).iloc[:50].merchant.tolist()\n",
    "test_rule_uid = pd.DataFrame(Test_trans[Test_trans['merchant'].isin(rule_code_1)].UID.unique())\n",
    "pred_data_rule = Test_tag.merge(test_rule_uid,left_on ='UID',right_on =0, how ='left')\n",
    "pred_data_rule['Tag'][(pred_data_rule[0]>0)] = 1\n",
    "pred_data_rule[['UID', 'Tag']].to_csv('../sub/sub+rule.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix the past result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = generate_test_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('../sub/all+neighbor+resample.csv')\n",
    "sub2 = pd.read_csv('../sub/stack_all+neighbor-day-time-daystd.csv')\n",
    "sub3 = pd.read_csv('../sub/all+neighbor-day-time-daystd-tw.csv')#all+neighbor-day+merchant.csv\n",
    "sub4 = pd.read_csv('../sub/all+neighbor-day+merchant.csv')\n",
    "sub5 = pd.read_csv('../sub/all+neighbor-day.csv')\n",
    "sub6 = pd.read_csv('../sub/all+neighbor-day-time1.csv')\n",
    "sub7 = pd.read_csv('../sub/all+neighbor-day-time-daystd-ip1subneighbor.csv')\n",
    "sub8 = pd.read_csv('../sub/mix.csv')\n",
    "sub9 = pd.read_csv('../sub/mix4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['Tag'] = (sub1['Tag']+sub2['Tag']+sub3['Tag']+sub4['Tag']+sub5['Tag']+sub6['Tag']+sub7['Tag']+sub8['Tag']+sub9['Tag'])/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'mix9'\n",
    "sub.to_csv('../sub/' + name + '.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
